# DataOps Inspector: Automated Data Quality & Model Drift Platform

A comprehensive platform for monitoring data quality, detecting model drift, and providing automated alerts for data and ML operations.

## ğŸš€ Features

### Data Pipeline & Quality Monitoring
- Upload/connect to CSV files or public datasets
- Automated ETL pipeline with Python/Pandas
- Real-time data quality checks (missing values, duplicates, schema drift)
- PostgreSQL storage for logs and results

### Data Quality Dashboard
- Interactive React dashboard with real-time metrics
- Data summary visualizations (row count, unique values)
- Issue tracking (nulls, duplicates, schema changes)
- Historical trend charts

### ML Model Deployment & Drift Monitoring
- Sample ML pipeline with scikit-learn
- Real-time performance tracking (accuracy, F1 score)
- Model drift detection using Evidently AI
- Automated alerts for performance degradation

### Notifications
- Email alerts for critical issues
- Web dashboard notifications
- Configurable alert thresholds

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚    â”‚   Backend API   â”‚    â”‚   Database      â”‚
â”‚   (React)       â”‚â—„â”€â”€â–ºâ”‚   (FastAPI)     â”‚â—„â”€â”€â–ºâ”‚   (PostgreSQL)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚   ML Pipeline   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚   (scikit-learn)â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ Tech Stack

- **Backend**: Python, FastAPI, Pandas, SQLAlchemy, scikit-learn, Evidently AI
- **Frontend**: React, Chart.js, Material-UI
- **Database**: PostgreSQL
- **Deployment**: Docker, Docker Compose
- **Notifications**: SMTP Email

## ğŸš€ Quick Start

### Prerequisites
- Docker and Docker Compose
- Git

### Installation

1. Clone the repository:
```bash
git clone <repository-url>
cd DataOpsInspector
```

2. Start the platform:
```bash
docker-compose up -d
```

3. Access the application:
- Frontend: http://localhost:3000
- Backend API: http://localhost:8000
- API Documentation: http://localhost:8000/docs

### Environment Variables

Copy `.env.example` to `.env` and configure:
```bash
cp .env.example .env
```

## ğŸ“Š Usage

### 1. Data Upload & Monitoring
- Navigate to the Data Quality section
- Upload a CSV file or connect to a data source
- View real-time quality metrics and issues

### 2. ML Model Deployment
- Go to the Model Monitoring section
- Deploy a sample model or upload your own
- Monitor performance and drift detection

### 3. Alerts & Notifications
- Configure alert thresholds in Settings
- Receive email notifications for critical issues
- View alerts in the dashboard

## ğŸ“ Project Structure

```
DataOpsInspector/
â”œâ”€â”€ backend/                 # FastAPI backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/            # API routes
â”‚   â”‚   â”œâ”€â”€ core/           # Core configurations
â”‚   â”‚   â”œâ”€â”€ models/         # Database models
â”‚   â”‚   â”œâ”€â”€ services/       # Business logic
â”‚   â”‚   â””â”€â”€ utils/          # Utilities
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ frontend/               # React frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/     # React components
â”‚   â”‚   â”œâ”€â”€ pages/          # Page components
â”‚   â”‚   â”œâ”€â”€ services/       # API services
â”‚   â”‚   â””â”€â”€ utils/          # Utilities
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ ml_pipeline/           # ML pipeline components
â”‚   â”œâ”€â”€ models/            # ML models
â”‚   â”œâ”€â”€ drift_detection/   # Drift detection logic
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ docker-compose.yml     # Docker orchestration
â”œâ”€â”€ .env.example          # Environment variables template
â””â”€â”€ README.md             # This file
```

## ğŸ”§ Development

### Backend Development
```bash
cd backend
pip install -r requirements.txt
uvicorn app.main:app --reload
```

### Frontend Development
```bash
cd frontend
npm install
npm start
```

### Database
```bash
# Using Docker
docker-compose up postgres

# Or connect directly
psql -h localhost -U dataops_user -d dataops_db
```

## ğŸ“ˆ Monitoring & Metrics

### Data Quality Metrics
- Missing value percentage
- Duplicate record count
- Schema change detection
- Data type validation
- Outlier detection

### Model Performance Metrics
- Accuracy, Precision, Recall, F1-Score
- Feature distribution drift
- Prediction drift
- Model performance trends

## ğŸš¨ Alerting

### Alert Types
- **Data Quality Alerts**: High missing values, schema changes
- **Model Drift Alerts**: Performance degradation, feature drift
- **System Alerts**: Pipeline failures, connection issues

### Alert Channels
- Email notifications
- Dashboard notifications
- Webhook support (future)

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ†˜ Support

For support and questions:
- Create an issue in the repository
- Check the documentation at `/docs`
- Review the API documentation at `http://localhost:8000/docs`

## ğŸ¯ Roadmap

- [ ] Advanced ML model support (TensorFlow, PyTorch)
- [ ] Real-time streaming data support
- [ ] Advanced visualization options
- [ ] Multi-tenant architecture
- [ ] REST API for external integrations
- [ ] Advanced alerting rules engine
- [ ] Data lineage tracking
- [ ] Automated model retraining 